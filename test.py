import os
import re
import json
import faiss
import torch
import gradio as gr
import numpy as np
from typing import List, Dict, Tuple
from jieba import lcut
from sklearn.metrics import accuracy_score, f1_score
from sentence_transformers import SentenceTransformer, util
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig
)


# ===================== 1. æ ¸å¿ƒé…ç½®ï¼ˆä»…éœ€ä¿®æ”¹2å¤„ï¼‰ =====================
class Config:
    LORA_MODEL_PATH = "./cpc_history_lora"  # ä½ çš„LoRAå¾®è°ƒåæ¨¡å‹è·¯å¾„ï¼ˆå¦‚./modelï¼‰
    QA_DATA_DIR = "./data"  #
    # ---------- å›ºå®šé…ç½®ï¼ˆä½œä¸šè¦æ±‚ï¼‰ ----------
    QA_DATA_PATH = f"{QA_DATA_DIR}/cpc_history_qa.json"  # é€‚é…ä½ dataç›®å½•çš„QAæ•°æ®ï¼ˆæ”¯æŒjson/csvï¼‰
    VECTOR_DB_PATH = f"{QA_DATA_DIR}/faiss_index.index"  # è‡ªåŠ¨æ„å»ºçš„å‘é‡åº“è·¯å¾„
    MAX_CONTEXT_LEN = 32768  # 32ké•¿ä¸Šä¸‹æ–‡
    CONFIDENCE_THRESH = 0.7  # ä½äºè¯¥å€¼æ‹’ç»å›ç­”
    MAX_HISTORY_TURNS = 5  # å¤šè½®å¯¹è¯ä¿ç•™5è½®
    RETRIEVE_TOP_K = 8  # æ£€ç´¢å¬å›æ•°ï¼ˆå¯è¿­ä»£è°ƒæ•´ï¼‰
    RERANK_TOP_K = 5  # é‡æ’åºåä¿ç•™æ•°


# ===================== 2. è¯»å–å¹¶æ¸…æ´—ä½ çš„QAæ•°æ® =====================
def load_qa_data(config: Config) -> List[Dict]:
    """è¯»å–ä½ dataç›®å½•ä¸‹çš„QAæ•°æ®ï¼Œè‡ªåŠ¨æ¸…æ´—é€‚é…RAG"""
    # æ”¯æŒjson/csvä¸¤ç§æ ¼å¼ï¼ˆè¦†ç›–å¸¸è§QAæ•°æ®æ ¼å¼ï¼‰
    qa_files = [f for f in os.listdir(config.QA_DATA_DIR) if f.endswith((".json", ".csv"))]
    if not qa_files:
        raise ValueError(f"âŒ åœ¨{config.QA_DATA_DIR}ç›®å½•æœªæ‰¾åˆ°QAæ•°æ®ï¼ˆjson/csvï¼‰")

    data_file = os.path.join(config.QA_DATA_DIR, qa_files[0])
    print(f"âœ… è¯»å–QAæ•°æ®ï¼š{data_file}")

    # è¯»å–æ•°æ®
    if data_file.endswith(".json"):
        with open(data_file, "r", encoding="utf-8") as f:
            raw_data = json.load(f)
    else:  # csv
        import pandas as pd
        df = pd.read_csv(data_file, encoding="utf-8")
        raw_data = df.to_dict("records")

    # æ•°æ®æ¸…æ´—ï¼ˆå»ç©ºã€å»é‡ã€æ ¼å¼åŒ–ï¼‰
    cleaned_data = []
    for idx, item in enumerate(raw_data):
        # é€‚é…å¸¸è§QAå­—æ®µåï¼ˆquestion/q | answer/a | sourceï¼‰
        question = item.get("question") or item.get("q") or ""
        answer = item.get("answer") or item.get("a") or ""
        source = item.get("source") or f"æ•°æ®-{idx}"  # è‡ªåŠ¨è¡¥å……æ¥æºï¼ˆä½œä¸šè¦æ±‚å¼•ç”¨ï¼‰

        # æ¸…æ´—ï¼šå»ç©ºã€å»ç‰¹æ®Šå­—ç¬¦
        question = re.sub(r"[\s\t\n]+", "", str(question)).strip()
        answer = re.sub(r"[\s\t\n]+", "", str(answer)).strip()

        if not question or not answer:
            continue

        # é•¿æ–‡æœ¬åˆ†å—ï¼ˆé€‚é…32kä¸Šä¸‹æ–‡ï¼‰
        content = f"é—®é¢˜ï¼š{question} ç­”æ¡ˆï¼š{answer}"
        chunks = []
        sentences = re.split(r"[ã€‚ï¼ï¼Ÿï¼›]", content)
        current_chunk = ""
        for sent in sentences:
            if len(current_chunk) + len(sent) < 2048:  # å•å—ä¸è¶…2048 tokens
                current_chunk += sent + "ã€‚"
            else:
                chunks.append(current_chunk.strip())
                current_chunk = sent + "ã€‚"
        if current_chunk:
            chunks.append(current_chunk.strip())

        cleaned_data.append({
            "question": question,
            "answer": answer,
            "source": source,
            "chunks": chunks
        })

    print(f"âœ… QAæ•°æ®æ¸…æ´—å®Œæˆï¼Œæœ‰æ•ˆæ•°æ®é‡ï¼š{len(cleaned_data)}æ¡ï¼ˆéœ€â‰¥5kï¼Œä¸è¶³ä¼šæç¤ºï¼‰")
    if len(cleaned_data) < 5000:
        print("âš ï¸ æ³¨æ„ï¼šå½“å‰æœ‰æ•ˆQAæ•°æ®ä¸è¶³5kæ¡ï¼Œå»ºè®®è¡¥å……æ•°æ®ä»¥æ»¡è¶³ä½œä¸šè¦æ±‚")
    return cleaned_data


# ===================== 3. è‡ªåŠ¨æ„å»º/åŠ è½½å‘é‡æ•°æ®åº“ =====================
class VectorDB:
    def __init__(self, config: Config):
        self.config = config
        self.embedding_model = SentenceTransformer("shibing624/text2vec-base-chinese")  # è½»é‡ä¸”æ•ˆæœå¥½
        self.index = None
        self.doc_map = {}  # å‘é‡ç´¢å¼•â†’æ–‡æ¡£æ˜ å°„

    def build(self, cleaned_data: List[Dict]):
        """åŸºäºä½ çš„QAæ•°æ®æ„å»ºFAISSå‘é‡åº“"""
        # æå–æ‰€æœ‰æ–‡æœ¬å—
        all_chunks = []
        for item in cleaned_data:
            for chunk in item["chunks"]:
                all_chunks.append({
                    "text": chunk,
                    "source": item["source"],
                    "answer": item["answer"]
                })

        # ç”Ÿæˆembedding
        texts = [item["text"] for item in all_chunks]
        embeddings = self.embedding_model.encode(
            texts, convert_to_numpy=True, normalize_embeddings=True
        )

        # æ„å»ºFAISSç´¢å¼•
        dim = embeddings.shape[1]
        self.index = faiss.IndexFlatL2(dim)
        self.index.add(embeddings)
        self.doc_map = {i: all_chunks[i] for i in range(len(all_chunks))}

        # ä¿å­˜å‘é‡åº“
        faiss.write_index(self.index, self.config.VECTOR_DB_PATH)
        print(f"âœ… å‘é‡åº“æ„å»ºå®Œæˆï¼Œå­˜å‚¨è·¯å¾„ï¼š{self.config.VECTOR_DB_PATH}")

    def load(self):
        """åŠ è½½å·²æ„å»ºçš„å‘é‡åº“"""
        if os.path.exists(self.config.VECTOR_DB_PATH):
            self.index = faiss.read_index(self.config.VECTOR_DB_PATH)
            # é‡æ–°æ„å»ºdoc_mapï¼ˆéœ€åŒæ­¥QAæ•°æ®ï¼‰
            cleaned_data = load_qa_data(self.config)
            all_chunks = []
            for item in cleaned_data:
                for chunk in item["chunks"]:
                    all_chunks.append({
                        "text": chunk,
                        "source": item["source"],
                        "answer": item["answer"]
                    })
            self.doc_map = {i: all_chunks[i] for i in range(len(all_chunks))}
            print(f"âœ… å‘é‡åº“åŠ è½½å®Œæˆ")
        else:
            raise ValueError("âŒ å‘é‡åº“ä¸å­˜åœ¨ï¼Œè‡ªåŠ¨å¼€å§‹æ„å»º...")

    def retrieve(self, query: str) -> List[Dict]:
        """æ£€ç´¢ç­–ç•¥ï¼šå‘é‡æ£€ç´¢ï¼ˆå¯è¿­ä»£æ›¿æ¢ä¸ºæ··åˆæ£€ç´¢ï¼‰"""
        # ç”ŸæˆæŸ¥è¯¢embedding
        query_emb = self.embedding_model.encode(
            query, convert_to_numpy=True, normalize_embeddings=True
        )

        # æ£€ç´¢Top-K
        distances, indices = self.index.search(
            query_emb.reshape(1, -1), self.config.RETRIEVE_TOP_K
        )

        # æ•´ç†ç»“æœï¼ˆå½’ä¸€åŒ–ç›¸ä¼¼åº¦ï¼‰
        results = []
        for dist, idx in zip(distances[0], indices[0]):
            if idx == -1:
                continue
            doc = self.doc_map.get(idx, {})
            results.append({
                "text": doc.get("text", ""),
                "answer": doc.get("answer", ""),
                "source": doc.get("source", ""),
                "score": 1 - (dist / 2)  # ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰
            })

        # é‡æ’åºåå–Top-K
        results = sorted(results, key=lambda x: x["score"], reverse=True)[:self.config.RERANK_TOP_K]
        return results


# ===================== 4. RAGæ ¸å¿ƒé€»è¾‘ï¼ˆæ»¡è¶³æ‰€æœ‰ä½œä¸šè¦æ±‚ï¼‰ =====================
class RAGSystem:
    def __init__(self, config: Config):
        self.config = config
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

        # åŠ è½½å‘é‡åº“ï¼ˆæ— åˆ™è‡ªåŠ¨æ„å»ºï¼‰
        self.vector_db = VectorDB(config)
        cleaned_data = load_qa_data(config)
        try:
            self.vector_db.load()
        except:
            self.vector_db.build(cleaned_data)

        # åŠ è½½ä½ çš„LoRAå¾®è°ƒæ¨¡å‹ï¼ˆé€‚é…é•¿ä¸Šä¸‹æ–‡ï¼‰
        print(f"âœ… åŠ è½½LoRAå¾®è°ƒæ¨¡å‹ï¼š{config.LORA_MODEL_PATH}")
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16
        )

        self.tokenizer = AutoTokenizer.from_pretrained(
            config.LORA_MODEL_PATH, trust_remote_code=True, padding_side="right"
        )
        self.model = AutoModelForCausalLM.from_pretrained(
            config.LORA_MODEL_PATH,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.bfloat16
        )

        # é•¿ä¸Šä¸‹æ–‡é€‚é…ï¼šè®¾ç½®pad_token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

    def calculate_confidence(self, query: str, retrieve_results: List[Dict]) -> float:
        """è®¡ç®—å›ç­”ç½®ä¿¡åº¦ï¼ˆä½äºé˜ˆå€¼åˆ™æ‹’ç»å›ç­”ï¼‰"""
        if not retrieve_results:
            return 0.0
        query_emb = self.vector_db.embedding_model.encode(query, normalize_embeddings=True)
        similarities = []
        for res in retrieve_results:
            res_emb = self.vector_db.embedding_model.encode(res["text"], normalize_embeddings=True)
            sim = util.cos_sim(query_emb, res_emb).item()
            similarities.append(sim)
        return np.mean(similarities)

    def build_prompt(self, query: str, retrieve_results: List[Dict], history: List[Tuple]) -> str:
        """æ„å»º32ké•¿ä¸Šä¸‹æ–‡Promptï¼ˆå«å¤šè½®å†å²+æ£€ç´¢ç»“æœï¼‰"""
        # æ‹¼æ¥å¤šè½®å¯¹è¯å†å²
        history_text = ""
        for q, a in history[-self.config.MAX_HISTORY_TURNS:]:
            history_text += f"ç”¨æˆ·ï¼š{q}\nåŠ©æ‰‹ï¼š{a}\n"

        # æ‹¼æ¥æ£€ç´¢ç»“æœï¼ˆå¸¦æ¥æºï¼‰
        retrieve_text = ""
        sources = set()
        for res in retrieve_results:
            retrieve_text += f"å‚è€ƒå†…å®¹ï¼š{res['text']}\nå‚è€ƒç­”æ¡ˆï¼š{res['answer']}\næ¥æºï¼š{res['source']}\n\n"
            sources.add(res["source"])

        # ä½œä¸šè¦æ±‚ï¼šæ‹’ç»ä¸ç¡®å®šå›ç­”+å¼•ç”¨æ¥æº+é•¿ä¸Šä¸‹æ–‡
        prompt = f"""
        ä½ æ˜¯é¢†åŸŸä¸“å®¶ï¼Œä¸¥æ ¼éµå®ˆä»¥ä¸‹è§„åˆ™å›ç­”é—®é¢˜ï¼š
        1. ä»…ä½¿ç”¨æä¾›çš„å‚è€ƒä¿¡æ¯å›ç­”ï¼Œä¸Šä¸‹æ–‡é•¿åº¦ä¸è¶…è¿‡{self.config.MAX_CONTEXT_LEN} tokensï¼Œä¸ç¼–é€ å†…å®¹ï¼›
        2. è‹¥å‚è€ƒä¿¡æ¯ä¸é—®é¢˜æ— å…³ï¼ˆç½®ä¿¡åº¦<{self.config.CONFIDENCE_THRESH}ï¼‰ï¼Œä»…å›å¤ï¼šâ€œæŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç¡®å®šè¯¥é—®é¢˜çš„ç­”æ¡ˆã€‚â€ï¼›
        3. å›ç­”æœ«å°¾å¿…é¡»æ ‡æ³¨å¼•ç”¨æ¥æºï¼Œæ ¼å¼ï¼šã€å¼•ç”¨æ¥æºï¼šæ¥æº1,æ¥æº2ã€‘ï¼›
        4. ä¿ç•™å¤šè½®å¯¹è¯çš„ä¸Šä¸‹æ–‡ä¸€è‡´æ€§ã€‚

        å¤šè½®å¯¹è¯å†å²ï¼š
        {history_text}

        å‚è€ƒä¿¡æ¯ï¼š
        {retrieve_text}

        å½“å‰é—®é¢˜ï¼š{query}
        å›ç­”ï¼š
        """
        return prompt, sources

    def chat(self, query: str, history: List[Tuple]) -> Tuple[str, List[Tuple]]:
        """æ ¸å¿ƒå¯¹è¯é€»è¾‘ï¼šæ£€ç´¢â†’ç½®ä¿¡åº¦åˆ¤æ–­â†’ç”Ÿæˆå›ç­”â†’å¼•ç”¨æ¥æº"""
        # 1. æ£€ç´¢ç›¸å…³å†…å®¹
        retrieve_results = self.vector_db.retrieve(query)

        # 2. ç½®ä¿¡åº¦åˆ¤æ–­ï¼ˆæ‹’ç»ä¸ç¡®å®šå›ç­”ï¼‰
        confidence = self.calculate_confidence(query, retrieve_results)
        if confidence < self.config.CONFIDENCE_THRESH:
            history.append((query, "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç¡®å®šè¯¥é—®é¢˜çš„ç­”æ¡ˆã€‚"))
            return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç¡®å®šè¯¥é—®é¢˜çš„ç­”æ¡ˆã€‚", history

        # 3. æ„å»ºPrompt
        prompt, sources = self.build_prompt(query, retrieve_results, history)

        # 4. é•¿ä¸Šä¸‹æ–‡ç”Ÿæˆ
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=self.config.MAX_CONTEXT_LEN
        ).to(self.device)

        outputs = self.model.generate(
            **inputs,
            max_new_tokens=1024,
            temperature=0.7,
            top_p=0.9,
            eos_token_id=self.tokenizer.eos_token_id,
            pad_token_id=self.tokenizer.pad_token_id
        )

        # 5. è§£æå›ç­”+æ·»åŠ å¼•ç”¨æ¥æº
        answer = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        answer = answer.split("å›ç­”ï¼š")[-1].strip()
        answer += f"\nã€å¼•ç”¨æ¥æºï¼š{','.join(sources)}ã€‘"

        # 6. æ›´æ–°å¤šè½®å†å²
        history.append((query, answer))
        return answer, history


# ===================== 5. è¯„ä¼°æ¨¡å—ï¼ˆä½œä¸šè¦æ±‚ï¼šå‡†ç¡®ç‡/å¼•ç”¨F1/å¹»è§‰ç‡ï¼‰ =====================
def evaluate_rag(rag_system: RAGSystem, test_data: List[Dict]) -> Dict:
    """è¯„ä¼°RAGç³»ç»Ÿæ€§èƒ½ï¼ˆå¯¹æ¯”åŸºçº¿/ä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
    y_true = []
    y_pred = []
    f1_list = []
    hallucination_count = 0

    # æŠ½æ ·100æ¡è¯„ä¼°ï¼ˆé¿å…è€—æ—¶è¿‡ä¹…ï¼‰
    test_data = test_data[:100]
    for item in test_data:
        query = item["question"]
        true_answer = item["answer"]
        true_source = {item["source"]}

        # ç”Ÿæˆå›ç­”
        retrieve_results = rag_system.vector_db.retrieve(query)
        confidence = rag_system.calculate_confidence(query, retrieve_results)

        if confidence < rag_system.config.CONFIDENCE_THRESH:
            pred_answer = "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç¡®å®šè¯¥é—®é¢˜çš„ç­”æ¡ˆã€‚"
        else:
            prompt, _ = rag_system.build_prompt(query, retrieve_results, [])
            inputs = rag_system.tokenizer(
                prompt, return_tensors="pt", padding=True, truncation=True
            ).to(rag_system.device)
            outputs = rag_system.model.generate(**inputs, max_new_tokens=1024)
            pred_answer = rag_system.tokenizer.decode(outputs[0], skip_special_tokens=True)
            pred_answer = pred_answer.split("å›ç­”ï¼š")[-1].strip()

        # å‡†ç¡®ç‡
        y_true.append(1 if true_answer in pred_answer else 0)
        y_pred.append(1 if pred_answer != "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç¡®å®šè¯¥é—®é¢˜çš„ç­”æ¡ˆã€‚" else 0)

        # å¼•ç”¨F1
        pred_sources = re.findall(r"ã€å¼•ç”¨æ¥æºï¼š(.*?)ã€‘", pred_answer)
        pred_sources = set(pred_sources[0].split(",") if pred_sources else [])
        tp = len(true_source & pred_sources)
        fp = len(pred_sources - true_source)
        fn = len(true_source - pred_sources)

        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
        f1_list.append(f1)

        # å¹»è§‰ç‡ï¼ˆæ— æ¥æºæ ‡æ³¨ä¸”éæ‹’ç»å›ç­”ï¼‰
        if "ã€å¼•ç”¨æ¥æºï¼š" not in pred_answer and pred_answer != "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•ç¡®å®šè¯¥é—®é¢˜çš„ç­”æ¡ˆã€‚":
            hallucination_count += 1

    # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
    accuracy = accuracy_score(y_true, y_pred)
    avg_f1 = np.mean(f1_list)
    hallucination_rate = hallucination_count / len(test_data)

    return {
        "å‡†ç¡®ç‡": round(accuracy, 4),
        "å¼•ç”¨F1å€¼": round(avg_f1, 4),
        "å¹»è§‰ç‡": round(hallucination_rate, 4)
    }


# ===================== 6. Gradio Web Demoéƒ¨ç½²ï¼ˆä½œä¸šè¦æ±‚ï¼‰ =====================
def main():
    # åˆå§‹åŒ–é…ç½®
    config = Config()

    # åŠ è½½QAæ•°æ®
    cleaned_data = load_qa_data(config)

    # åˆå§‹åŒ–RAGç³»ç»Ÿ
    rag_system = RAGSystem(config)

    # è¯„ä¼°ï¼ˆè¿­ä»£ä¼˜åŒ–æ—¶å¯¹æ¯”åŸºçº¿/ä¼˜åŒ–ç‰ˆæœ¬ï¼‰
    print("\nğŸ“Š å¼€å§‹è¯„ä¼°RAGç³»ç»Ÿæ€§èƒ½...")
    eval_results = evaluate_rag(rag_system, cleaned_data)
    print(f"ğŸ“Š è¯„ä¼°ç»“æœï¼š{eval_results}")

    # æ„å»ºWeb Demo
    with gr.Blocks(title="LoRA+RAGé¢†åŸŸé—®ç­”ç³»ç»Ÿ") as demo:
        gr.Markdown("# ğŸ¯ LoRAå¾®è°ƒ+RAGé¢†åŸŸé—®ç­”ç³»ç»Ÿ")
        gr.Markdown(f"ğŸ“‹ ä½œä¸šè¦æ±‚è¾¾æ ‡é¡¹ï¼š32ké•¿ä¸Šä¸‹æ–‡ | å¤šè½®å¯¹è¯ | å¼•ç”¨æ¥æº | æ‹’ç»ä¸ç¡®å®šå›ç­”")
        gr.Markdown(f"ğŸ“Š è¯„ä¼°ç»“æœï¼š{eval_results}")

        chatbot = gr.Chatbot(label="å¤šè½®å¯¹è¯çª—å£", height=500)
        query_input = gr.Textbox(label="è¯·è¾“å…¥ä½ çš„é—®é¢˜", placeholder="è¾“å…¥é¢†åŸŸé—®é¢˜...")
        clear_btn = gr.Button("æ¸…ç©ºå¯¹è¯")

        # å¯¹è¯å‡½æ•°
        def respond(query, history):
            answer, history = rag_system.chat(query, history)
            return "", history

        # ç»‘å®šäº‹ä»¶
        query_input.submit(respond, [query_input, chatbot], [query_input, chatbot])
        clear_btn.click(lambda: None, None, chatbot, queue=False)

    # å¯åŠ¨Demoï¼ˆæœ¬åœ°è®¿é—®ï¼šhttp://localhost:7860ï¼‰
    print("\nğŸš€ Web Demoå¯åŠ¨ä¸­... è®¿é—®åœ°å€ï¼šhttp://localhost:7860")
    demo.launch(server_name="0.0.0.0", server_port=7860)


if __name__ == "__main__":
    main()