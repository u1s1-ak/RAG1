import streamlit as st
import os

os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
from langchain_community.document_loaders import TextLoader, UnstructuredFileLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig
import torch
from tqdm import tqdm
import glob

# ==========================================
# é…ç½®åŒºåŸŸï¼ˆé’ˆå¯¹å…šå²æ•°æ®ä¼˜åŒ–ï¼‰
# ==========================================
ST_TITLE = "å…šå²çŸ¥è¯† RAG é—®ç­”ç³»ç»Ÿ"

# æ•°æ®è·¯å¾„é…ç½®ï¼ˆä¿®æ”¹ä¸ºä½ çš„å…šå²ä¹¦ç±è·¯å¾„ï¼‰
TXT_FOLDER = "./data"  # å­˜æ”¾å…šå²ç›¸å…³txtæ–‡ä»¶
SUPPORTED_EXTENSIONS = ["*.txt"]  # æ”¯æŒå¤šç§æ ¼å¼
# æœ¬åœ° Qwen3-0.6-Instruct è·¯å¾„
LOCAL_MODEL_PATH = "/root/model/Qwen/Qwen3-0___6B"
# åµŒå…¥æ¨¡å‹ä¼˜åŒ–ä¸ºæ›´é€‚åˆä¸­æ–‡å…šå²å†…å®¹
EMBEDDING_MODEL = "BAAI/bge-large-zh-v1.5"  # å¯¹ä¸­æ–‡æ”¯æŒæ›´å¥½[6]
# å‘é‡åº“æŒä¹…åŒ–ç›®å½•
VECTOR_DB_PATH = "./chroma_db_party_history"

# æ–‡æœ¬åˆ†å‰²å‚æ•°ä¼˜åŒ–ï¼ˆé’ˆå¯¹å…šå²æ–‡çŒ®ç‰¹ç‚¹ï¼‰
CHUNK_SIZE = 600  # å…šå²æ–‡çŒ®é€šå¸¸ç»“æ„æ¸…æ™°ï¼Œé€‚å½“å‡å°chunkå¤§å°
CHUNK_OVERLAP = 80
RETRIEVE_COUNT = 5  # æ£€ç´¢æ•°é‡è°ƒæ•´


# ==========================================
# åˆå§‹åŒ– RAG ç³»ç»Ÿï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
# ==========================================
@st.cache_resource
def initialize_rag_system():
    """
    åˆå§‹åŒ–å…šå²RAGç³»ç»Ÿï¼Œé’ˆå¯¹å…šå²æ–‡çŒ®ç‰¹ç‚¹è¿›è¡Œä¼˜åŒ–
    """
    # 1. æ£€æŸ¥æ•°æ®æ–‡ä»¶å¤¹å¹¶æŸ¥æ‰¾æ”¯æŒçš„æ–‡ä»¶
    if not os.path.exists(TXT_FOLDER):
        return None, f"æ•°æ®æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {TXT_FOLDER}"

    # æŸ¥æ‰¾æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
    data_files = []
    for extension in SUPPORTED_EXTENSIONS:
        data_files.extend(glob.glob(os.path.join(TXT_FOLDER, extension)))

    if not data_files:
        return None, f"æ–‡ä»¶å¤¹ {TXT_FOLDER} ä¸­æ²¡æœ‰æ‰¾åˆ°æ”¯æŒçš„æ–‡æ¡£æ–‡ä»¶(txt/md/pdf)"

    st.info(f"å‘ç° {len(data_files)} ä¸ªå…šå²æ–‡æ¡£æ–‡ä»¶ï¼Œæ­£åœ¨åŠ è½½...")

    # 2. åŠ è½½æ–‡æ¡£ï¼ˆä¼˜åŒ–é”™è¯¯å¤„ç†ï¼‰
    docs = []
    failed_files = []

    for file_path in tqdm(data_files, desc="åŠ è½½å…šå²æ–‡æ¡£"):
        try:
            if file_path.endswith('.pdf'):
                # å¯¹äºPDFæ–‡ä»¶ä½¿ç”¨æ›´å¼ºå¤§çš„åŠ è½½å™¨
                loader = UnstructuredFileLoader(file_path, strategy="fast")
            else:
                loader = TextLoader(file_path, encoding="utf-8")

            file_docs = loader.load()
            # ä¸ºæ¯ä¸ªæ–‡æ¡£æ·»åŠ å…ƒæ•°æ®ï¼Œè®°å½•æ¥æºæ–‡ä»¶
            for doc in file_docs:
                doc.metadata["source"] = os.path.basename(file_path)
            docs.extend(file_docs)
        except Exception as e:
            failed_files.append((os.path.basename(file_path), str(e)))
            continue

    if failed_files:
        st.warning(f"éƒ¨åˆ†æ–‡ä»¶åŠ è½½å¤±è´¥: {[f[0] for f in failed_files]}")

    if not docs:
        return None, "æ‰€æœ‰æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼å’Œç¼–ç "

    st.success(f"æˆåŠŸåŠ è½½ {len(docs)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")

    # 3. æ–‡æœ¬åˆ‡åˆ†ä¼˜åŒ–ï¼ˆé’ˆå¯¹å…šå²æ–‡çŒ®ç‰¹ç‚¹ï¼‰
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
        length_function=len,
        separators=["\n## ", "\n# ", "\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›"],  # ä¸­æ–‡å‹å¥½åˆ†éš”ç¬¦
    )
    splits = text_splitter.split_documents(docs)
    st.info(f"åˆ‡åˆ†ä¸º {len(splits)} ä¸ªæ–‡æœ¬å—")

    # 4. åµŒå…¥æ¨¡å‹ä¼˜åŒ–
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={'device': 'cuda'},
        encode_kwargs={
            'normalize_embeddings': True,
            'batch_size': 32  # ä¼˜åŒ–æ‰¹å¤„ç†å¤§å°
        }
    )

    # 5. æ„å»ºæˆ–åŠ è½½å‘é‡åº“ï¼ˆæ·»åŠ é›†åˆåç§°é¿å…å†²çªï¼‰
    if os.path.exists(VECTOR_DB_PATH):
        st.info("æ£€æµ‹åˆ°å·²æœ‰å‘é‡åº“ï¼Œç›´æ¥åŠ è½½...")
        vectorstore = Chroma(
            persist_directory=VECTOR_DB_PATH,
            embedding_function=embeddings,
            collection_name="party_history_collection"
        )
    else:
        st.info("æ­£åœ¨æ„å»ºå…šå²çŸ¥è¯†å‘é‡åº“ï¼ˆé¦–æ¬¡è¿è¡Œè¾ƒæ…¢ï¼‰...")
        vectorstore = Chroma.from_documents(
            documents=splits,
            embedding=embeddings,
            persist_directory=VECTOR_DB_PATH,
            collection_name="party_history_collection"
        )
        st.success("å…šå²çŸ¥è¯†å‘é‡åº“æ„å»ºå®Œæˆå¹¶å·²ä¿å­˜ï¼")

    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": RETRIEVE_COUNT}
    )

    # 6. åŠ è½½æ¨¡å‹
    if not os.path.exists(LOCAL_MODEL_PATH):
        return None, f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {LOCAL_MODEL_PATH}"

    tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_PATH, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        LOCAL_MODEL_PATH,
        device_map="auto",
        torch_dtype=torch.bfloat16,
        trust_remote_code=True,
        # load_in_4bit=True,   # å¦‚æ˜¾å­˜ä¸å¤Ÿå¯å¼€å¯ï¼ˆéœ€ pip install bitsandbytesï¼‰
    )

    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=1024,
        temperature=0.3,
        top_p=0.9,
        do_sample=True,
        repetition_penalty=1.1
    )

    # æ­£ç¡®æ–¹å¼ï¼šå…ˆåŒ…è£…æˆ HuggingFacePipelineï¼Œå†ç”¨ ChatHuggingFace
    llm_pipeline = HuggingFacePipeline(pipeline=pipe)

    llm = ChatHuggingFace(
        llm=llm_pipeline,  # â† å¿…é¡»ç”¨ llm= å‚æ•°
        tokenizer=tokenizer,
        streaming=True
    )

    # 7. Promptæ¨¡æ¿ä¼˜åŒ–ï¼ˆé’ˆå¯¹å…šå²é—®ç­”ç‰¹ç‚¹ï¼‰
    template = """
ä½ æ˜¯ä¸€ä¸ªå…šå²ç ”ç©¶ä¸“å®¶ï¼Œè¯·æ ¹æ®ä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ï¼Œå‡†ç¡®ã€è¯¦å°½åœ°å›ç­”ç”¨æˆ·å…³äºå…šå²çš„é—®é¢˜ã€‚

å…šå²çŸ¥è¯†å…·æœ‰ä¸¥è‚ƒæ€§å’Œå‡†ç¡®æ€§è¦æ±‚ï¼Œè¯·ç¡®ä¿ï¼š
1. å›ç­”è¦åŸºäºäº‹å®ï¼Œå‡†ç¡®å¼•ç”¨å†å²äº‹ä»¶çš„æ—¶é—´ã€åœ°ç‚¹å’Œäººç‰©
2. å¯¹äºé‡è¦å†å²äº‹ä»¶å’Œå†³ç­–ï¼Œè¦ä½“ç°å…¶å†å²èƒŒæ™¯å’Œæ„ä¹‰
3. å¦‚æœä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è¶³ï¼Œè¯·æ˜ç¡®è¯´æ˜å¹¶å»ºè®®æŸ¥é˜…æƒå¨å…šå²èµ„æ–™
4. å›ç­”è¦ä½“ç°å…šå²æ•™è‚²çš„ä¸¥è‚ƒæ€§å’Œæ•™è‚²æ„ä¹‰

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

è¯·æ ¹æ®ä»¥ä¸Šä¸Šä¸‹æ–‡æä¾›ä¸“ä¸šã€å‡†ç¡®çš„å…šå²çŸ¥è¯†å›ç­”ï¼š
"""
    prompt = ChatPromptTemplate.from_template(template)

    # 8. RAG Chain
    rag_chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | prompt
            | llm
            | StrOutputParser()
    )

    return rag_chain, f"ç³»ç»Ÿå°±ç»ªï¼çŸ¥è¯†åº“åŒ…å« {len(data_files)} ä¸ªå…šå²æ–‡æ¡£"


# ==========================================
# Streamlit ç•Œé¢ï¼ˆä¼˜åŒ–ç”¨æˆ·ä½“éªŒï¼‰
# ==========================================
st.set_page_config(
    page_title=ST_TITLE,
    page_icon="ğŸ‡¨ğŸ‡³",  # æ”¹ä¸ºæ›´ç¬¦åˆå…šå²ä¸»é¢˜çš„å›¾æ ‡
    layout="wide"
)

st.title(ST_TITLE)
st.markdown("### åŸºäºæœ¬åœ°å¤§æ¨¡å‹çš„å…šå²çŸ¥è¯†æ™ºèƒ½é—®ç­”ç³»ç»Ÿ")

with st.sidebar:
    st.header("ğŸ“Š ç³»ç»ŸçŠ¶æ€")
    with st.spinner("æ­£åœ¨åˆå§‹åŒ–å…šå²RAGç³»ç»Ÿ..."):
        rag_chain, msg = initialize_rag_system()

    if rag_chain:
        st.success("âœ… RAG ç³»ç»Ÿå·²å°±ç»ª")
        st.info(msg)
        st.info(f"ğŸ§  æ¨¡å‹: æœ¬åœ° Qwen3-0.6B\n\nğŸ“š åµŒå…¥æ¨¡å‹: {EMBEDDING_MODEL}")

        # æ·»åŠ ä½¿ç”¨æç¤º
        st.markdown("---")
        st.header("ğŸ’¡ ä½¿ç”¨æç¤º")
        st.info("""
        æ‚¨å¯ä»¥è¯¢é—®å…³äºå…šå²çš„ä»¥ä¸‹å†…å®¹ï¼š
        - é‡è¦å†å²äº‹ä»¶
        - å…šçš„å†æ¬¡ä»£è¡¨å¤§ä¼š
        - é‡è¦å†å²äººç‰©
        - å…šçš„ç†è®ºå‘å±•
        - å†å²ç»éªŒå’Œæ•™è®­
        """)
    else:
        st.error(f"âŒ åˆå§‹åŒ–å¤±è´¥: {msg}")
        st.stop()

    if st.button("ğŸ—‘ï¸ æ¸…é™¤å¯¹è¯å†å²"):
        st.session_state.messages = []
        st.rerun()

# åˆå§‹åŒ–å¯¹è¯å†å²
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "æ‚¨å¥½ï¼æˆ‘æ˜¯å…šå²çŸ¥è¯†é—®ç­”åŠ©æ‰‹ï¼Œå¯ä»¥ä¸ºæ‚¨è§£ç­”å…³äºä¸­å›½å…±äº§å…šå†å²çš„å„ç§é—®é¢˜ã€‚"}
    ]

# æ˜¾ç¤ºå†å²å¯¹è¯
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ç”¨æˆ·è¾“å…¥åŒºåŸŸ
if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨æƒ³äº†è§£çš„å…šå²ç›¸å…³é—®é¢˜ï¼Œä¾‹å¦‚ï¼š'ä¸­å›½å…±äº§å…šæˆç«‹çš„å†å²èƒŒæ™¯æ˜¯ä»€ä¹ˆï¼Ÿ'"):
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # ç”ŸæˆåŠ©æ‰‹å›å¤
    with st.chat_message("assistant"):
        placeholder = st.empty()
        full_response = ""

        try:
            # æµå¼è¾“å‡ºå“åº”
            for chunk in rag_chain.stream(prompt):
                full_response += chunk
                placeholder.markdown(full_response + "â–Œ")
            placeholder.markdown(full_response)

        except Exception as e:
            error_msg = f"ç”Ÿæˆå›ç­”æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
            st.error(error_msg)
            full_response = "æŠ±æ­‰ï¼Œæˆ‘åœ¨å¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°äº†é—®é¢˜ã€‚è¯·ç¨åå†è¯•æˆ–å°è¯•é‡æ–°è¡¨è¿°æ‚¨çš„é—®é¢˜ã€‚"

    # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
    st.session_state.messages.append({"role": "assistant", "content": full_response})

# æ·»åŠ é¡µè„šä¿¡æ¯
st.markdown("---")
st.caption("ğŸ” æœ¬ç³»ç»ŸåŸºäºæ£€ç´¢å¢å¼ºç”Ÿæˆ(RAG)æŠ€æœ¯æ„å»ºï¼Œèƒ½å¤Ÿæ ¹æ®æä¾›çš„å…šå²èµ„æ–™æä¾›å‡†ç¡®çš„é—®ç­”æœåŠ¡[6](@ref)")